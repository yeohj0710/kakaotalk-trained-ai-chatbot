# PORTABLE_STATE.txt
Single Source of Truth for resuming this project on another machine.

[Current Architecture]
- Pretrained base model + LoRA SFT
- Run name default: room_lora_qwen3b
- Main config: configs/sft.yaml

[Required for training resume]
1) Code + config
- configs/sft.yaml
- .env

2) Data
- data/raw/inbox/*.txt
- data/sft/train.jsonl
- data/sft/val.jsonl
- data/sft/stats.json

3) Checkpoints
- checkpoints_lora/<run_name>/checkpoint-*
- checkpoints_lora/<run_name>/adapter_latest
- checkpoints_lora/<run_name>/adapter_best
- checkpoints_lora/<run_name>/status.json

[Required for inference only]
- configs/sft.yaml
- .env
- checkpoints_lora/<run_name>/adapter_best (or adapter_latest)

[Environment variables]
- CHATBOT_PASSWORD=...

[One-line commands]
- Archive old outputs: python -m chatbot.sft_ops archive
- Organize raw txt:   python -m chatbot.sft_ops organize
- Preprocess:         python -m chatbot.sft_ops preprocess
- Train/resume:       python -m chatbot.sft_ops train
- Reply:              python -m chatbot.sft_ops reply "테스트"
- Chat:               python -m chatbot.sft_ops chat
- Smoke test:         python -m chatbot.sft_ops smoke

[Resume behavior]
- Stop: Ctrl+C or STOP file in run dir
- Resume: same `train` command; auto resumes from latest checkpoint

[Practical note]
- First run downloads base model from Hugging Face.
- If 4bit quantization is unavailable, loader may fallback and use more VRAM.
