project:
  run_name: room_lora_qwen25_3b_base
  seed: 42

paths:
  raw_glob: data/raw/inbox/*.txt
  output_dir: data/sft
  train_jsonl: data/sft/train.jsonl
  val_jsonl: data/sft/val.jsonl
  cpt_train_jsonl: data/sft/cpt_train.jsonl
  cpt_val_jsonl: data/sft/cpt_val.jsonl
  preview_json: data/sft/preview.json
  stats_json: data/sft/stats.json
  checkpoints_root: checkpoints_lora
  stop_file_name: STOP
  status_json: checkpoints_lora/{run_name}/status.json

data:
  val_ratio: 0.02
  include_system: false
  shuffle_before_split: false
  session_gap_minutes: 180
  context_turns: 24
  min_context_turns: 3
  sample_stride: 1
  merge_same_speaker: true
  merge_gap_minutes: 2
  max_merged_chars: 360
  min_message_chars: 2
  min_target_chars: 8
  max_message_chars: 360
  drop_low_signal: true
  mask_urls: true
  mask_numbers: false
  drop_media_only: true
  drop_summary_artifacts: true
  summary_bullet_min_count: 1
  max_examples_per_split: 0

cpt_data:
  window_messages: 64
  stride_messages: 16
  min_messages: 10
  min_chars: 120
  max_chars: 2200
  use_speaker_prefix: true

pipeline:
  enabled: true
  run_cpt_first: true
  bootstrap_sft_from_cpt: true
  cpt_run_name_suffix: "_cpt"
  skip_cpt_if_sft_has_checkpoint: true
  require_cpt_complete_before_sft: true

prompt:
  system: |
    너는 카카오톡 단톡방에 참여한 사람처럼 말한다.
    상담원 같은 과한 공손체와 설명문을 피하고, 실제 채팅처럼 짧고 자연스럽게 답해라.
    필요하면 구어체나 비속어를 과장 없이 사용할 수 있다.
  task: |
    아래 대화 흐름을 보고 다음에 이어질 법한 답변 1개를 작성하라.
    화자 라벨 없이 답변 문장만 출력하라.
  response_one_line: true

model:
  base_model: Qwen/Qwen2.5-3B
  trust_remote_code: true
  use_fast_tokenizer: true
  local_files_only: false
  load_in_4bit: true
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  torch_dtype: bfloat16
  attn_implementation: sdpa
  gradient_checkpointing: false

lora:
  r: 32
  alpha: 64
  dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

cpt_training:
  max_seq_len: 768
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  grad_accum_steps: 4
  learning_rate: 0.00015
  weight_decay: 0.01
  warmup_ratio: 0.02
  lr_scheduler_type: cosine
  max_steps: 300000
  num_train_epochs: 1
  eval_steps: 1000
  save_steps: 1000
  logging_steps: 20
  save_total_limit: 20
  max_grad_norm: 1.0
  bf16: true
  fp16: false
  tf32: true
  dataloader_num_workers: 2
  load_best_model_at_end: true
  early_stopping_patience: 0

training:
  max_seq_len: 768
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  grad_accum_steps: 4
  learning_rate: 0.00012
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  max_steps: 120000
  num_train_epochs: 1
  eval_steps: 5000
  save_steps: 500
  logging_steps: 20
  save_total_limit: 8
  max_grad_norm: 1.0
  bf16: true
  fp16: false
  tf32: true
  dataloader_num_workers: 2
  load_best_model_at_end: false
  early_stopping_patience: 0

generation:
  max_new_tokens: 96
  do_sample: true
  temperature: 0.85
  top_p: 0.92
  top_k: 50
  repetition_penalty: 1.05
  no_repeat_ngram_size: 4
  avoid_summary_artifacts: true
  avoid_self_echo: true
  avoid_repetitive_output: true
  max_mentions: 0
  max_history_turns: 8
  include_bot_history: true
  max_bot_history_turns: 8
  min_reply_chars: 8
  regen_attempts: 4
  one_line: true
  max_chars: 220
  use_chat_template: false

smoke:
  prompts:
    - 오늘 뭐함
    - 아까 말한 거 요약해봐
    - 그럼 결론 뭐로 갈까

security:
  require_password: true
  password_env: CHATBOT_PASSWORD
