project:
  run_name: room_lora_qwen3b
  seed: 42

paths:
  raw_glob: data/raw/inbox/*.txt
  output_dir: data/sft
  train_jsonl: data/sft/train.jsonl
  val_jsonl: data/sft/val.jsonl
  preview_json: data/sft/preview.json
  stats_json: data/sft/stats.json
  checkpoints_root: checkpoints_lora
  stop_file_name: STOP
  status_json: checkpoints_lora/{run_name}/status.json

data:
  val_ratio: 0.02
  include_system: false
  shuffle_before_split: false
  session_gap_minutes: 180
  context_turns: 18
  min_context_turns: 3
  sample_stride: 2
  merge_same_speaker: true
  merge_gap_minutes: 3
  max_merged_chars: 420
  min_message_chars: 2
  min_target_chars: 10
  max_message_chars: 420
  drop_low_signal: true
  mask_urls: true
  mask_numbers: false
  drop_media_only: true
  max_examples_per_split: 0

prompt:
  system: |
    너는 특정 개인의 자아를 주장하는 AI가 아니다.
    목표는 단톡방 구성원처럼 자연스럽고 상황 맞는 답변 한 개를 말하는 것이다.
    과장된 설정, 역할놀이, 자기정체성 주장, 장황한 설명을 피하고 실제 카톡 대화처럼 답해라.
  task: |
    아래 대화 흐름을 보고 다음에 이어질 법한 답변을 한 개 작성하라.
    화자 이름/역할 라벨 없이 답변 문장만 출력하라.
  response_one_line: true

model:
  base_model: Qwen/Qwen2.5-3B-Instruct
  trust_remote_code: true
  use_fast_tokenizer: true
  local_files_only: false
  load_in_4bit: true
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  torch_dtype: bfloat16
  attn_implementation: sdpa
  gradient_checkpointing: true

lora:
  r: 64
  alpha: 128
  dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

training:
  max_seq_len: 1024
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  grad_accum_steps: 16
  learning_rate: 0.0002
  weight_decay: 0.0
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  max_steps: 300000
  num_train_epochs: 1
  eval_steps: 500
  save_steps: 500
  logging_steps: 20
  save_total_limit: 6
  max_grad_norm: 1.0
  bf16: true
  fp16: false
  dataloader_num_workers: 0
  gradient_accumulation_plugin: auto
  early_stopping_patience: 20

generation:
  max_new_tokens: 120
  do_sample: true
  temperature: 0.72
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.08
  max_history_turns: 14
  min_reply_chars: 8
  regen_attempts: 2
  one_line: true
  max_chars: 240

smoke:
  prompts:
    - 오늘 뭐하냐
    - 아까 얘기한거 한줄로 정리해봐
    - 그럼 결론 뭐로 가는게 맞음?

security:
  require_password: true
  password_env: CHATBOT_PASSWORD
